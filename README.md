Deep Learning of Narrative
===========================

- Input docs as json lines
  lazy or one line at a time? latter done.
   
- tokenize
  capitalization (char-rnn annotation trick?), punctuation, numbers, 
  light touch done.

- Frames TODO
	- output direclty to frametrain if using sdr pro-tem re-engineer later. 
	- What does word2vec do?
  
- EMBEDDINGS word2vec vs. SDR
	- why is word2vec hand-wavy about semantic locality
	- SDR is clear should we use SDR? downsampling SDRs/or sparse rep as they are very big input space
	- what does word2vec do?

- TERMS rare vs. common words filtering/folding?

- LSTM word sequence model
	- char-RNN vs. word-RNN word on the street is word for performance and profit.

- HOLD ON -- what do we need this LANGUAGE model for?
  what can we do with it? What can we ask it? 
  How does it help?


   
